<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Exploring the Usefulness of Adding Auxiliary Preprocessed Image Layers With Convolutional Neural Networks</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
                    <h3>Exploring the Usefulness of Adding Auxiliary Preprocessed Image Layers With Convolutional Neural Networks</h3>
                    <p>
                        <br>
                        Jordan Goetze
                        <br>
                        Dr. Anne Denton
                        <br>
                        <br>
                        <small>
                            Dept. of Computer Science<br>
                            North Dakota State University<br>
                            Fargo, North Dakota 58103<br>
                            <br>
                            jordan.goetze@ndsu.edu<br>
                            anne.denton@ndsu.edu
                        </small>
                    </p>
                </section>
                <section>
                    <section>
                        <h2>Terminology</h2>
                    </section>
                    <section>
                        <h3>Terminology</h3>
                        <p>
                            <b>Per-pixel image classifications</b> 
                        </p>
                        <p class="fragment">Useful for:</p>
                        <ul>
                            <li class="fragment">Scene labeling</li>
                            <li class="fragment">Inferring relationships in an image.</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Terminology</h3>
                        <p>
                            <b>Land-Use Classification</b>
                        </p>
                        <p class="fragment">Potential uses:</p>
                        <ul>
                            <li class="fragment">
                                Approximating crop yields by year
                            </li>
                            <li class="fragment">
                                Tracking changes in land use 
                            </li>
                            <li class="fragment">
                                Tracking changes in forestry and vegetation
                            </li>
                        </ul>
                    </section>
                    <section>
                        <h3>Terminology</h3>
                        <b>Orthoimagery:</b>
                        <small>"Fixes" various displacements such as building tilt and scale variations caused by terrain relief.
                        </small>
                        <div>
                        <image style="height:50%; width:50%" data-src="images/ortho_imagery_example.png">
                    </section>
                    <section>
                        <h3>Terminology</h3>
                        <b>Convolutional Neural Network (CNN)</b>
                        <p class="fragment">Useful for:</p>
                        <ul>
                            <li class="fragment">Scene labeling</li>
                            <li class="fragment">Image classification</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Terminology</h3>
                        <b>Auxiliary Preprocessed Image Layer</b> 
                        <p class="fragment">Example: Normalized Difference Vegetation Index(NDVI)</p>
                        <p class="fragment">$ NDVI = \frac{NIR-RED}{NIR+RED} $</p>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Introduction</h2>
                    </section>
                    <section>
                        <h4>Introduction: What do CNNs learn?</h4>
                        <hr>
                        <p class="fragment">Are there certain kinds of patterns they cannot learn, or learn too slowly to be effective?</p>
                        <hr>
                        <p class="fragment">Does the inclusion of image layers that are generated from the original source image help or hurt the model?</p>
                        <hr>
                    </section>
                    <section>
                        <h4>Introduction: Technology, Imagery and Data</h4>
                        <ul>
                            <li class="fragment">Cost of satellites and drones is decreasing.</li>
                            <li class="fragment">Cost of orthoimagery decreasing</li>
                            <li class="fragment">Amount of availiable orthoimagry increasing</li>
                            <li class="fragment">Amount and quality of labeled or annotated orthoimagery has not kept pace</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Introduction: Availiable Data</h4>
                        <b>National Agricultural Imagery Program (NAIP):</b>
                        <ul>
                            <li class="fragment">Provides imagery spanning the majority of the continental United States</li>
                            <li class="fragment">1 meter Ground Sample Distance (GSD)</li>
                            <li class="fragment"> 
                                <span style="color:red">Red</span>, 
                                <span style="color:green">Green</span>, 
                                <span style="color:blue">Blue</span>, and 
                                <span style="color:red">NIR</span> image layers</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Introduction: Availiable Data</h4>
                        <b>National Agricultural Statistics Service (NASS) Land-Use Classifications:</b>
                        <ul>
                            <li class="fragment">Continental United States</li>
                            <li class="fragment">Low resolution accuracy compared to NAIP imagery. 
                                <ul>
                                    <li class="fragment">1 NASS pixel represents a 50 square meter area in the NAIP Imagery</li>
                                </ul>
                            </li>
                            <li class="fragment">Poor quality classifications</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Introduction: NASS - Mislabeled Pixels</h4>
                        <img style="height:75%; width:50%" data-src="images/poor_labels_1.png" />
                    </section>
                    <section>
                        <h4>Introduction: NASS - Clipped Organic Features</h4>
                        <img style="height:75%; width:50%" data-src="images/poor_labels_2.png" />
                    </section>
                    <section>
                        <h4>Introduction: NASS - Poor Representation of Fine Features</h4>
                        <img style="height:50%; width:50%" data-src="images/poor_labels_3.png" />
                    </section>
                    <section>
                        <h4>Introduction: Goal</h4>
                        <p>Test whether the inclusion of several different auxilary image layers, generated from the base RGB, NIR, and NDVI layers, serves to help or hurt the classification accuracy and qualitative quality of the model's classifications.</p>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Previous Work</h2>
                    </section>
                    <section>
                        <h4>Previous Work: Selecting a CNN Model</h2>
                        <ul>
                            <li class="fragment">Focus on roads or buildings.</li>
                            <li class="fragment">Little research into identifying agricultural features</li>
                            <li class="fragment">Per-pixel classifications of orthoimagery fall under the realm of scene recognition.
                                <ul><li class="fragment">Orthoimagery features tend to lack well defined boundaries</li></ul> 
                            </li>
                            <li class="fragment">The SegNet model provides a relatively efficient and effective approach to scene recognition.</li>
                        <ul>
                    </section>
                    <section>
                        <h4>Previous Work: SegNet</h4>
                        <ul>
                            <li class="fragment">Deep Convolutional Encoder-Decoder Network</li>
                            <li class="fragment">Produces good results when applied to CamVid dataset.</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Previous Work: SegNet Demo</h4>
                            <iframe width="1200" height="600" data-src="https://www.youtube.com/embed/CxanE_W46ts" frameborder="0" allowfullscreen></iframe>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Data Set Preprocessing</h2>
                    </section>
                    <section>
                        <h4>Data Set Preprocessing: Data Sets</h4>
                        <ul>
                            <li class="fragment"><b>Images:</b> NAIP Imagery</li>
                            <li class="fragment"><b>Ground Truths:</b> NASS Land-use classifications</li>
                        </ul>
                        <p class="fragment">Images clipped into 256x256 swatches</p>
                    </section>
                    <section>
                        <h4>Data Set Preprocessing: Data Sets</h4>
                        <p class="fragment">Ground Truth data simplified into two classes: Water, and Not-Water</p>
                        <table class="fragment">
                            <tr>
                                <th>Not-Water</th>
                                <th>Water</th>
                            </tr>
                            <tr>
                                <td>93%</td>
                                <td>7%</td>
                            </tr>
                        </table>
                    </section>
                    <section>
                        <h4>Data Set Preprocessing: Gradient Image Layer</h4>
                        <p class="fragment">
                            Using a sliding window, a gradient is computed by taking the scaled value between 0 and 255 of 
                            the largest difference in pixel intensity.
                        </p>
                        <br>
                        <p class="fragment">
                        The sliding window operates over <b>one</b> of the original image layers to produce an intensity value 
                            for each pixel in the image, excluding a small border.
                        </p>
                    </section>
                    <section>
                        <h4>Data Set Preprocessing: Gradient Image Layer</h4>
                        <ul>
                            <li class="fragment"><b>Input Image Layer:</b> NDVI</li>
                            <li class="fragment"><b>Window Size:</b> 8x8 pixels</li>
                            <li class="fragment"><b>Output Image Size:</b> 248x248 pixels</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Data Set Preprocessing: Regression Image Layer</h4>
                        <p class="fragment">
                            Using a sliding window, take the slope of the line calculated by taking the linear-regression between
                            two bands.
                        </p>
                        <br>
                        <p class="fragment">
                        The sliding window operates over <b>two</b> of the original image layers to produce an intensity value 
                            for each pixel in the image, excluding a small border.
                        </p>
                    </section>
                    <section>
                        <h4>Data Set Preprocessing: Regression Image Layer</h4>
                        <p class="fragment">
                            Implementation based off of the paper <em>Multi-scalar Analysis of Geospacial Agricultural 
                            Data for Sustainabiliy</em> which introduces a means of allowing larger sliding windows without 
                            the computational cost of scanning for them.
                        </p>
                    </section>
                    <section>
                        <img data-src="images/dentons_paper.png">
                    </section>
                    <section>
                        <h4>Data Set Preprocessing: Regression Image Layer</h4>
                        <ul>
                            <li class="fragment"><b>Input Image Layer 1:</b> Red</li>
                            <li class="fragment"><b>Input Image Layer 2:</b> NIR</li>
                            <li class="fragment"><b>Window Size:</b> 8x8 pixels</li>
                            <li class="fragment"><b>Output Image Size:</b> 248x248 pixels</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Data Set Preprocessing: Data Set Size</h4>
                        <p class="fragment">
                            Dataset of 2,000 images.
                        </p>
                        <p class="fragment">
                            Generating the aux images takes ~1.5 hours per type.
                        </p>
                        <p class="fragment">
                            Training model takes 4-6 hours for 3 epochs.
                        </p>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Model</h2>
                    </section>
                    <section>
                        <h4>Model: SegNet</h4>
                        <img style="width: 200%" data-src="images/network.png"/>
                        <p>Kernel Size 7x7</p>
                    </section>
                    <section>
                        <h4>Model: SegNet</h4>
                        <img style="width: 200%" data-src="images/feature_window_layers.png"/>
                        <p>Max pool + Indice Unraveling</p>    
                        <p>Example with 4 down-sample &amp; up-sample layers</p>
                    </section>
                    <section>
                        <h4>Model: Custom SegNet Variant</h4>
                        <img data-src="images/network.png"/>
                        <p>Kernel Size 3x3</p>
                        <p>3 down-sample &amp; up-sample layers</p>
                    </section>
                    <section>
                        <h4>Model Variants</h4>
                        <ul>
                            <li class="fragment"><b>Control (No Aux Image Layer):</b> Our control model. The base Custom SegNet Variant previously described. Takes RGB, NIR, and NDVI image layers as input.</li>
                            <li class="fragment"><b>Gradient Model:</b> A modified version of the control that takes an additional layer generated via the Gradient process previously described.</li>
                            <li class="fragment"><b>Regression Model:</b> A modified version of the control that takes an additional layer generated via the Regression process previously described.</li>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Training &amp; Evaluation</h2>
                    </section>
                    <section>
                        <h4>Training</h4>
                        <ul>
                            <li class="fragment">Trained on 90% of availiable image swatches
                                <ul>
                                    <li class="fragment">1,800 image swatches</li>
                                </ul>
                            </li>
                            <li class="fragment">Batches of 15</li>
                            <li class="fragment">3 Epochs</li>
                            <li class="fragment">Checkpoints are saved every 100 steps</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Evaluation</h4>
                        <ul>
                            <li class="fragment">Evaluation is done on the remaining 10% of availiable image swatches
                                <ul>
                                    <li class="fragment">200 image swatches</li>
                                </ul>
                            </li>
                            <li class="fragment">The checkpoint with the highest evaluation accuracy is selected</li>
                        </ul>
                    </section>
                    <section>
                        <h4>Special Note</h4>
                        <p class="fragment">Training and Testing sets are generated once and then remain the same for all models.</p>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Analysis</h2>
                    </section>
                    <section>
                        <h4>Analysis: Evaluation Accuracy</h4>
                        <table>
                            <tr>
                                <th>Model Type</th>
                                <th>Accuracy</th>
                            </tr>
                            <tr>
                                <td>Control (No Aux Image Layer)</td>
                                <td>92.3680%</td>
                            </tr>
                            <tr>
                                <td>Regression</td>
                                <td>86.0488%</td>
                            </tr>
                            <tr>
                                <td>Gradient</td>
                                <td class="fragment highlight-blue">93.3380%</td>
                            </tr>
                        </table>
                        <p class="fragment"><span class="fragment highlight-red">93%</span> of the data set is Not-Water</p>
                    </section>
                    <section>
                        <h4>Analysis: Per-Class Evaluation Accuracy</h4>
                        <table>
                            <tr>
                                <th>Model Type</th>
                                <th>Not-Water Accuracy</th>
                                <th>Water Accuracy</th>
                            </tr>
                            <tr>
                                <td>Control (No Aux Image Layer)</td>
                                <td>96.8892%</td>
                                <td>37.6272%</td>
                            </tr>
                            <tr>
                                <td>Regression</td>
                                <td>89.7641%</td>
                                <td class="fragment highlight-blue">40.8237%</td>
                            </tr>
                            <tr>
                                <td>Gradient</td>
                                <td class="fragment highlight-blue">98.5329%</td>
                                <td>30.2602%</td>
                            </tr>
                        </table>
                        <p><span class="fragment highlight-blue">93%</span> of the data set is Not-Water</p>
                    </section>
                    <section>
                        <h4>Analysis: Gradient Model</h4>
                        <p>Per-class Not-Water accuracy at the cost of per-class water accuracy.</p>
                        <img data-src="images/grad_104_loss_of_water_accuracy.png"/>
                    </section>
                    <section>
                        <h4>Analysis: Regression Model</h4>
                        <p>Per-class Water Accuracy at the cost of per-class not-water accuracy.</p>
                    </section>
                    <section>
                        <div>
                            <img style="vertical-align:middle"  data-src="images/control_114_missing_secondary_water_source_labeled.png" onload="this.width/=1.2;this.onload=null;"/>
                        </div>
                        <div>
                            <img style="vertical-align:middle"  data-src="images/regression_114_missing_secondary_water_source_labeled.png" onload="this.width/=1.2;this.onload=null;"/>
                        </div>
                        <div>
                            <img style="vertical-align:middle"  data-src="images/gradient_114_missing_secondary_water_source_labeled.png" onload="this.width/=1.2;this.onload=null;"/>
                        </div>
                    </section>
                    <section>
                        <h4>Analysis: Regression Model</h4>
                        <p>The regression model's water classifications tend to respond strongly to places where there is lots of vegetation along the coast of a water body.</p>
                    </section>
                    <section>
                        <div>
                            <img style="vertical-align:middle"  data-src="images/control_12_strong_waterline_response_labeled.png" onload="this.width/=1.2;this.onload=null;"/>
                        </div>
                        <div>
                            <img style="vertical-align:middle"  data-src="images/reg_12_strong_waterline_response_labeled.png" onload="this.width/=1.2;this.onload=null;"/>
                        </div>
                        <div>
                            <img style="vertical-align:middle"  data-src="images/grad_12_strong_waterline_response_labeled.png" onload="this.width/=1.2;this.onload=null;"/>
                        </div>
                    </section>
                    <section>
                        <h4>Analysis: Regression Model</h4>
                        <table>
                            <tr class="fragment">
                                <td>Control</td>
                                <td>Underestimates water areas</td>
                            </tr>
                            <tr class="fragment">
                                <td>Gradient</td>
                                <td>Further underestimates water areas</td>
                            </tr>
                            <tr class="fragment">
                                <td>Regression</td>
                                <td>Vastly overestimates water areas</td>
                            </tr>
                        </table>
                    </section>
                </section>
                <section>
                    <h2>Conclusion</h2>
                </section>
                <section>
                    <h2>Thank You</h2>
                </section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
                transition: 'convex',
				controls: true,
				progress: true,
				history: true,
				center: true,

				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/math/math.js', async: true }
				],

                math: {
                    mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
                    config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
                },

			});

		</script>
	</body>
</html>
